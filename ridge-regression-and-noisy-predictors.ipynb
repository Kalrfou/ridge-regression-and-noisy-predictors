{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Noise to Regression Predictors is Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Noise to Regression Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tikhonov Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Statement of the Problem\n",
    "\n",
    "The linear regression problem is to find a vector $\\hat \\beta$ that minmizes\n",
    "\n",
    "$$ \\hat \\beta = argmin_\\beta \\left| y - X \\beta \\right|^2 $$\n",
    "\n",
    "Where $X$ is a matrix of predictiors (the design matrix) and $y$ is a vector of responses.  The idea is that we would like to predict $y$ from $X$ by using a linear function in the columns of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our setup, we scale each entry of $X$ by a small amount of gaussian noise before regressing:\n",
    "\n",
    "$$ x_{ij} \\rightarrow \\epsilon_{ij} x_{ij} $$\n",
    "\n",
    "where $\\epsilon \\sim N(1, \\sigma)$.\n",
    "\n",
    "Of course, we get a different line for each choice of random $\\epsilon$; we are interested in what happens *on average*.  That is, we are interested in the solution vector $\\beta$ that is the *expectation* under this process\n",
    "\n",
    "$$ \\hat \\beta \\sim argmin_\\beta E_G \\left[ \\left| y - (G * X) \\beta  \\right|^2 \\right] $$\n",
    "\n",
    "In this equation, $G$ represents a matrix of random gaussian noise, the $*$ operator is elementwise multiplication of matricies, and $E_G$ marginalizes out the contributions of the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by expanding out the quantity inside the expectation:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\left| y - (G * X) \\beta  \\right|^2 &= \\left( y - (G * X) \\beta \\right)^t \\left( y - (G * X) \\beta \\right) \\\\\n",
    "&= y^t y - 2 y^t (G * X) \\beta + \\beta^t (G * X)^t (G * X) \\beta\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Quadratic Term\n",
    "\n",
    "We focus on the last term for the moment, say we name\n",
    "\n",
    "$$ M = (G * X)^t (G * X) $$\n",
    "\n",
    "Now a single entry in this matrix is\n",
    "\n",
    "$$ m_{ij} = \\sum_{k} \\epsilon_{ki} \\epsilon_{kj} x_{ki} x_{kj} $$\n",
    "\n",
    "which in expectation is\n",
    "\n",
    "$$ E_G \\left[ m_{ij} \\right] = \\sum_{k} E \\left[ \\epsilon_{ki} \\epsilon_{kj} \\right] x_{ki} x_{kj} $$\n",
    "\n",
    "If $i \\neq j$, then $\\epsilon_{ki}$ and $\\epsilon_{kj}$ are independent random varaibles both drawn from a $N(1, \\sigma)$, so\n",
    "\n",
    "$$ E \\left[ \\epsilon_{ki} \\epsilon_{kj} \\right] = 1 \\ \\text{for} i \\neq j $$\n",
    "\n",
    "For $i = j$, we can compute\n",
    "\n",
    "$$ E \\left[ \\epsilon_{ki}^2 \\right] = E \\left[ (\\epsilon_{ki} - 1)^2 + 2 \\epsilon_{ki} - 1 \\right] = \\sigma^2 + 2 - 1 = \\sigma^2 + 1 $$\n",
    "\n",
    "So all together\n",
    "\n",
    "$$ E \\left[ \\epsilon_{ki} \\epsilon_{kj} \\right] = \\begin{cases} \\sigma^2 + 1 & \\text{if} \\ i = j \\\\ 1 & \\text{otherwise} \\end{cases} $$\n",
    "\n",
    "This means that\n",
    "\n",
    "$$ E[M] = \\left( \\mathbb{1} + diag(\\sigma^2) \\right) * X^t X  = X^t X + diag(\\sigma^2) X^t X $$\n",
    "\n",
    "Where $\\mathbb{1}$ is a matrix with a $1$ in every entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it All Together\n",
    "\n",
    "Now we can compute the expectation of our entire quantity\n",
    "\n",
    "$$\\begin{align*}\n",
    "E \\left[ \\left| y - (G * X) \\beta  \\right|^2 \\right] &= E \\left[ y^t y - 2 y^t (G * X) \\beta + \\beta^t (G * X)^t (G * X) \\right] \\\\\n",
    "&= y^t y - 2 y^t (E[G] * X) \\beta + \\beta^t E \\left[ M \\right] \\beta \\\\\n",
    "&= y^t y - 2 y^t X \\beta + \\beta^t X^t X \\beta + \\beta^2 diag(\\sigma^2) X^t X \\beta \\\\\n",
    "&= \\left| y - X \\beta \\right|^2 + \\beta^2 diag(\\sigma^2) X^t X \\beta \\\\\n",
    "&= \\left| y - X \\beta \\right|^2 + \\sigma \\left| \\Gamma \\beta \\right|^2\n",
    "\\end{align*}$$\n",
    "\n",
    "Where $\\Gamma = \\sqrt{ diag \\left( X^t X \\right) }$.\n",
    "\n",
    "So, overall, our original problem can be restated as\n",
    "\n",
    "$$ \\hat \\beta \\sim argmin_\\beta \\left( \\left| y - X \\beta \\right|^2 + \\sigma \\left| \\Gamma \\beta \\right|^2 \\right) $$\n",
    "\n",
    "Which we recognise as linear regression with a tikhonov regularization term, with the regularization strength depending on the amount of noise we add to each predictor: more noise results in stronger regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "In ridge regression, we always ensure that our predictors are standardized before regressing.  That is, we ensure that $diag(X^t X) = I$.  If we impose this assumption to our resulting regularization problem, we get\n",
    "\n",
    "$$ \\hat \\beta \\sim argmin_\\beta \\left( \\left| y - X \\beta \\right|^2 + \\sigma \\left|\\beta \\right|^2 \\right) $$\n",
    "\n",
    "That is, under the usual assumptions of unit variance, our add-noise procedure is *in expectation* equivelent to ridge regression with a regularization strenght *equal* to the standard deviation of the noise we add to each predictor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
